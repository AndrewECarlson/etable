// Copyright (c) 2019, The Emergent Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

package etensor

import (
	"errors"
	"strconv"

	"github.com/apache/arrow/go/arrow"
	"github.com/apache/arrow/go/arrow/array"
	"github.com/apache/arrow/go/arrow/memory"
	"github.com/apache/arrow/go/arrow/tensor"
	"github.com/emer/emergent/bitslice"
	"github.com/goki/ki/ints"
	"github.com/goki/ki/kit"
)

{{range .In}}

// {{.Name}} is an n-dim array of {{.Type}}s.
type {{.Name}} struct {
	Shape
	Values []{{.Type}}
	Nulls bitslice.Slice
}

// New{{.Name}} returns a new n-dimensional array of {{.Type}}s.
// If strides is nil, row-major strides will be inferred.
// If names is nil, a slice of empty strings will be created.
// Nulls are initialized to nil.
func New{{.Name}}(shape, strides []int, names []string) *{{.Name}} {
	tsr := &{{.Name}}{}
	tsr.SetShape(shape, strides, names)
	tsr.Values = make([]{{or .Type}}, tsr.Len())
	return tsr
}

// New{{.Name}}Shape returns a new n-dimensional array of {{.Type}}s.
// Using shape structure instead of separate data.
// Nulls are initialized to nil.
func New{{.Name}}Shape(shape *Shape) *{{.Name}} {
	tsr := &{{.Name}}{}
	tsr.CopyShape(shape)
	tsr.Values = make([]{{or .Type}}, tsr.Len())
	return tsr
}

func (tsr *{{.Name}}) DataType() arrow.DataType { return &arrow.{{.Name}}Type{} }
func (tsr *{{.Name}}) Value(i []int)  {{or .Type}} { j := tsr.Offset(i); return tsr.Values[j] }
func (tsr *{{.Name}}) Value1D(i int)  {{or .Type}} { return tsr.Values[i] }
func (tsr *{{.Name}}) Set(i []int, val {{or .Type}})  { j := tsr.Offset(i); tsr.Values[j] = val }
func (tsr *{{.Name}}) IsNull(i []int) bool {
	if tsr.Nulls == nil {
		return false
	}
	j := tsr.Offset(i)
	return tsr.Nulls.Index(j)
}
func (tsr *{{.Name}}) SetNull(i []int, nul bool) {
	if tsr.Nulls == nil {
		tsr.Nulls = bitslice.Make(tsr.Len(), 0)
	}
	j := tsr.Offset(i)
	tsr.Nulls.Set(j, nul)
}

func (tsr *{{.Name}}) FloatVal(i []int) float64 { j := tsr.Offset(i); return float64(tsr.Values[j]) }
func (tsr *{{.Name}}) SetFloat(i []int, val float64)  { j := tsr.Offset(i); tsr.Values[j] = {{or .Type}}(val) }

func (tsr *{{.Name}}) StringVal(i []int) string { j := tsr.Offset(i); return kit.ToString(tsr.Values[j]) }
func (tsr *{{.Name}}) SetString(i []int, val string) {
	if fv, err := strconv.ParseFloat(val, 64); err == nil {
		j := tsr.Offset(i);
		tsr.Values[j] = {{or .Type}}(fv)
	}
}

func (tsr *{{.Name}}) FloatVal1D(off int) float64 { return float64(tsr.Values[off]) }
func (tsr *{{.Name}}) SetFloat1D(off int, val float64)  { tsr.Values[off] = {{or .Type}}(val) }

func (tsr *{{.Name}}) StringVal1D(off int) string { return kit.ToString(tsr.Values[off]) }
func (tsr *{{.Name}}) SetString1D(off int, val string) {
	if fv, err := strconv.ParseFloat(val, 64); err == nil {
		tsr.Values[off] = {{or .Type}}(fv)
	}
}

// AggFloat applies given aggregation function to each element in the tensor, using float64
// conversions of the values.  init is the initial value for the agg variable.  returns final
// aggregate value
func (tsr *{{.Name}}) 	AggFloat(fun func(val float64, agg float64) float64, ini float64) float64 {
	ln := tsr.Len()
	ag := ini
	for j := 0; j < ln; j++ {
		val := float64(tsr.Values[j])
		ag = fun(val, ag)
	}
	return ag
}


// EvalFloat applies given function to each element in the tensor, using float64
// conversions of the values, and puts the results into given float64 slice, which is
// ensured to be of the proper length
func (tsr *{{.Name}}) EvalFloat(fun func(val float64) float64, res *[]float64) {
	ln := tsr.Len()
	if len(*res) != ln {
		*res = make([]float64, ln)
	}
	for j := 0; j < ln; j++ {
		val := float64(tsr.Values[j])
		(*res)[j] = fun(val)
	}
}

// UpdtFloat applies given function to each element in the tensor, using float64
// conversions of the values, and writes the results back into the same tensor values
func (tsr *{{.Name}}) 	UpdtFloat(fun func(val float64) float64) {
	ln := tsr.Len()
	for j := 0; j < ln; j++ {
		val := float64(tsr.Values[j])
		tsr.Values[j] = {{or .Type}}(fun(val))
	}
}

// Clone creates a new tensor that is a copy of the existing tensor, with its own
// separate memory -- changes to the clone will not affect the source.
func (tsr *{{.Name}}) Clone() *{{.Name}} {
	csr := New{{.Name}}Shape(&tsr.Shape)
	copy(csr.Values, tsr.Values)
	if tsr.Nulls != nil {
		csr.Nulls = tsr.Nulls.Clone()
	}
	return csr
}

// CloneTensor creates a new tensor that is a copy of the existing tensor, with its own
// separate memory -- changes to the clone will not affect the source.
func (tsr *{{.Name}}) CloneTensor() Tensor {
	return tsr.Clone()
}

// SetShape sets the shape params, resizing backing storage appropriately
func (tsr *{{.Name}}) SetShape(shape, strides []int, names []string) {
	tsr.Shape.SetShape(shape, strides, names)
	nln := tsr.Len()
	if cap(tsr.Values) >= nln {
		tsr.Values = tsr.Values[0:nln]
	} else {
		nv := make([]{{or .Type}}, nln)
		copy(nv, tsr.Values)
		tsr.Values = nv
	}
}

// AddRows adds n rows (outer-most dimension) to RowMajor organized tensor.
func (tsr *{{.Name}}) AddRows(n int) {
	if !tsr.IsRowMajor() {
		return
	}
	rows, cells := tsr.RowCellSize()
	nln := (rows + n) * cells
	tsr.Shape.Shp[0] += n
	if cap(tsr.Values) >= nln {
		tsr.Values = tsr.Values[0:nln]
	} else {
		nv := make([]{{or .Type}}, nln)
		copy(nv, tsr.Values)
		tsr.Values = nv
	}
}

// SetNumRows sets the number of rows (outer-most dimension) in a RowMajor organized tensor.
func (tsr *{{.Name}}) SetNumRows(rows int) {
	if !tsr.IsRowMajor() {
		return
	}
	rows = ints.MaxInt(1, rows) // must be > 0
	_, cells := tsr.RowCellSize()
	nln := rows * cells
	tsr.Shape.Shp[0] = rows
	if cap(tsr.Values) >= nln {
		tsr.Values = tsr.Values[0:nln]
	} else {
		nv := make([]{{or .Type}}, nln)
		copy(nv, tsr.Values)
		tsr.Values = nv
	}
}

// SubSlice returns a new tensor as a sub-slice of the current one, incorporating the given number
// of dimensions (0 < subdim < NumDims of this tensor).  Only valid for row or column major layouts.
// subdim are the inner, contiguous dimensions (i.e., the final dims in RowMajor and the first ones in ColMajor).
// offs are offsets for the outer dimensions (len = NDims - subdim) for the subslice to return.
// The new tensor points to the values of the this tensor (i.e., modifications will affect both).
// Use Clone() method to separate the two.
func (tsr *{{.Name}}) SubSlice(subdim int, offs []int) (*{{.Name}}, error) {
	nd := tsr.NumDims()
	od := nd - subdim
	if od <= 0 {
		return nil, errors.New("SubSlice number of sub dimensions was >= NumDims -- must be less")
	}
	if tsr.IsRowMajor() {
		stsr := &{{.Name}}{}
		stsr.SetShape(tsr.Shp[od:], nil, tsr.Nms[od:]) // row major def
		sti := make([]int, nd)
		copy(sti, offs)
		stoff := tsr.Offset(sti)
		sln := stsr.Len()
		stsr.Values = tsr.Values[stoff:stoff+sln]
		return stsr, nil
	} else if tsr.IsColMajor() {
		stsr := &{{.Name}}{}
		stsr.SetShape(tsr.Shp[:subdim], nil, tsr.Nms[:subdim])
		stsr.Strd = ColMajorStrides(stsr.Shp)
		sti := make([]int, nd)
		for i := subdim; i < nd; i++ {
			sti[i] = offs[i-subdim]
		}
		stoff := tsr.Offset(sti)
		sln := stsr.Len()
		stsr.Values = tsr.Values[stoff:stoff+sln]
		return stsr, nil
	}
	return nil, errors.New("SubSlice only valid for RowMajor or ColMajor tensors")
}

// Label satisfies the gi.Labeler interface for a summary description of the tensor
func (tsr *{{.Name}}) Label() string {
	return fmt.Sprintf("{{.Name}}: %s", tsr.Shape.String())
}
	
// String satisfies the fmt.Stringer interface for string of tensor data
func (tsr *{{.Name}}) String() string {
	str := tsr.Label()
	sz := len(tsr.Values)
	if sz > 1000 {
		return str
	}
	for i := 0; i < sz; i++ {
		idx := tsr.Index(i)
		for j := 1; j < len(idx); j++ {
			if idx[j] == 0 {
				str += "\n["
				for k := 0; k < len(idx); k++ {
					str += fmt.Sprintf("%d", idx[k])
					if k < len(idx)-1 {
						str += ","
					}
				}
				str += "]: "
				break
			}
		}
		str += tsr.StringVal1D(i) + " "
	}
	str += "\n"
	return str
}
	
// ToArrow returns the apache arrow equivalent of the tensor
func (tsr *{{.Name}}) ToArrow() *tensor.{{.Name}} {
	bld := array.New{{.Name}}Builder(memory.DefaultAllocator)
	if tsr.Nulls != nil {
		bld.AppendValues(tsr.Values, tsr.Nulls.ToBools())
	} else {
		bld.AppendValues(tsr.Values, nil)
	}
	vec := bld.New{{.Name}}Array()
	return tensor.New{{.Name}}(vec.Data(), tsr.Shape64(), tsr.Strides64(), tsr.DimNames())
}

// FromArrow intializes this tensor from an arrow tensor of same type
// cpy = true means make a copy of the arrow data, otherwise it directly
// refers to its values slice -- we do not Retain() on that data so it is up
// to the go GC and / or your own memory management policies to ensure the data
// remains intact!
func (tsr *{{.Name}}) FromArrow(arw *tensor.{{.Name}}, cpy bool) {
	nms := make([]string, arw.NumDims()) // note: would be nice if it exposed DimNames()
	for i := range nms {
		nms[i] = arw.DimName(i)
	}
	tsr.SetShape64(arw.Shape(), arw.Strides(), nms)
	if cpy {
		vls := arw.{{.Name}}Values()
		tsr.Values = make([]{{or .Type}}, tsr.Len())
		copy(tsr.Values, vls)
	} else {
		tsr.Values = arw.{{.Name}}Values()
	}
	// note: doesn't look like the Data() exposes the nulls themselves so it is not
	// clear we can copy the null values -- nor does it seem that the tensor class
	// exposes it either!  https://github.com/apache/arrow/issues/3496
	// nln := arw.Data().NullN()
	// if nln > 0 {
	// }
}


{{- end}}

// New returns a new Tensor of given type, using our Type specifier which is
// isomorphic with arrow.Type
func New(dtype Type, shape, strides []int, names []string) Tensor {
	aty := arrow.Type(dtype)
	switch aty {
{{- range .In}}
	case arrow.{{.DataType}}:
		return New{{.Name}}(shape, strides, names)
{{- end}}
	case arrow.STRING:
		return NewString(shape, strides, names)
	case arrow.BOOL:
		return NewBits(shape, strides, names)
	}
	return nil
}

